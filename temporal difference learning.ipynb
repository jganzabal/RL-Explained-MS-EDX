{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n",
    "- No necesita terminar el episodio\n",
    "- Free model\n",
    "- tiene bias pero mas varianza\n",
    "- Depende fuertemente de los valores iniciales por el bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gridworld_mdp as gw   # defines the MDP for a 4x4 gridworld\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "n_states = gw.get_state_count()\n",
    "print(n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_lazy_programmer(gw, policy):\n",
    "    state = np.random.randint(gw.get_state_count())\n",
    "    states_and_rewards = [(state, 0)]\n",
    "    G = 0\n",
    "    while True:\n",
    "        actions = gw.get_available_actions(state)\n",
    "        action = actions[np.random.choice(len(actions), p=policy)]\n",
    "        transitions = gw.get_transitions(state=state, action=action)\n",
    "        trans_probs = []\n",
    "        for _, _, probabiliity in transitions:\n",
    "            trans_probs.append(probabiliity)\n",
    "        state, reward, _ = transitions[np.random.choice(len(trans_probs), p=trans_probs)]\n",
    "        states_and_rewards.append((state, reward))\n",
    "        if state == 0:\n",
    "            break\n",
    "    return states_and_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evel_TD0_lazy_programmer(gw, policy, gamma, alpha = 1, episodes=1000):\n",
    "    np.random.seed(seed)\n",
    "    V = np.random.random(n_states)\n",
    "    for it in range(episodes):\n",
    "        states_and_rewards = play_episode_lazy_programmer(gw, policy)\n",
    "        for t in range(len(states_and_rewards)-1):\n",
    "            s, _  = states_and_rewards[t]\n",
    "            s2, r = states_and_rewards[t+1]\n",
    "            V[s] = V[s] + alpha*(r  +  gamma * V[s2] - V[s])\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00868931, -5.74312299, -7.03889983, -7.71758402, -5.05904968,\n",
       "       -6.72712856, -7.22716084, -7.40129112, -7.25137599, -7.31717797,\n",
       "       -6.87068725, -6.20788345, -7.7570374 , -7.21293701, -5.10723724])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "policy = [0.25, 0.25, 0.25, 0.25]\n",
    "gamma = 0.9\n",
    "policy_evel_TD0_lazy_programmer(gw, policy, gamma, alpha = 0.05, episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(V, gw, policy, gamma, alpha = 1):\n",
    "    state = np.random.randint(n_states)\n",
    "    G = 0\n",
    "    while True:\n",
    "        actions = gw.get_available_actions(state)\n",
    "        action = actions[np.random.choice(len(actions), p=policy)]\n",
    "        transitions = gw.get_transitions(state=state, action=action)\n",
    "        trans_probs = []\n",
    "        for _, _, probabiliity in transitions:\n",
    "            trans_probs.append(probabiliity)\n",
    "        next_state, reward, _ = transitions[np.random.choice(len(trans_probs), p=trans_probs)]\n",
    "        V[state] = V[state] + alpha*(reward  +  gamma * V[next_state] - V[state]) # error = reward  +  gamma * V[next_state] - V[s]\n",
    "        state = next_state\n",
    "        if state == 0:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evel_TD0(gw, policy, gamma, alpha = 1, episodes=1000):\n",
    "    V = np.random.random(n_states)\n",
    "    for i in range(episodes):\n",
    "        V = play_episode(V, gw, policy, gamma, alpha)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00868931, -5.74312299, -7.03889983, -7.71758402, -5.05904968,\n",
       "       -6.72712856, -7.22716084, -7.40129112, -7.25137599, -7.31717797,\n",
       "       -6.87068725, -6.20788345, -7.7570374 , -7.21293701, -5.10723724])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "policy = [0.25, 0.25, 0.25, 0.25]\n",
    "gamma = 0.9\n",
    "policy_evel_TD0(gw, policy, gamma, alpha = 0.05, episodes=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array([ 0.00868931, -5.74312299, -7.03889983, -7.71758402, -5.05904968,\n",
    "       -6.72712856, -7.22716084, -7.40129112, -7.25137599, -7.31717797,\n",
    "       -6.87068725, -6.20788345, -7.7570374 , -7.21293701, -5.10723724])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
